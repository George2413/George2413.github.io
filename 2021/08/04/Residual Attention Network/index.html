<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Residual Attention Network | lllllcy</title><meta name="keywords" content="注意力模型"><meta name="author" content="George"><meta name="copyright" content="George"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1、论文全名 — Residual Attention Network for Image Classification2、&amp;emsp;整体结构 左图:一个例子展示了特征和注意力面具之间的相互作用。 右图:举例说明在我们的网络中，不同的特征有不同的对应注意面具。  软掩码分支结构 &amp;emsp;&amp;emsp;&amp;emsp;掩码分支包含**快速前馈扫描**和**自上而下的反馈**步骤。**前者快速采集整幅">
<meta property="og:type" content="article">
<meta property="og:title" content="Residual Attention Network">
<meta property="og:url" content="http://example.com/2021/08/04/Residual%20Attention%20Network/index.html">
<meta property="og:site_name" content="lllllcy">
<meta property="og:description" content="1、论文全名 — Residual Attention Network for Image Classification2、&amp;emsp;整体结构 左图:一个例子展示了特征和注意力面具之间的相互作用。 右图:举例说明在我们的网络中，不同的特征有不同的对应注意面具。  软掩码分支结构 &amp;emsp;&amp;emsp;&amp;emsp;掩码分支包含**快速前馈扫描**和**自上而下的反馈**步骤。**前者快速采集整幅">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva3.sinaimg.cn/large/ad66c4fbgy1g3ew33gjqij21hc0u07ap.jpg">
<meta property="article:published_time" content="2021-08-04T04:16:21.000Z">
<meta property="article:modified_time" content="2021-08-05T04:26:17.350Z">
<meta property="article:author" content="George">
<meta property="article:tag" content="注意力模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva3.sinaimg.cn/large/ad66c4fbgy1g3ew33gjqij21hc0u07ap.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/08/04/Residual%20Attention%20Network/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Residual Attention Network',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-08-05 12:26:17'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="lllllcy" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/mann.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://tva3.sinaimg.cn/large/ad66c4fbgy1g3ew33gjqij21hc0u07ap.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">lllllcy</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Residual Attention Network</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-04T04:16:21.000Z" title="发表于 2021-08-04 12:16:21">2021-08-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-08-05T04:26:17.350Z" title="更新于 2021-08-05 12:26:17">2021-08-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/">论文阅读（第一层级）</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7%EF%BC%89/">注意力模型（第二层级）</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Residual Attention Network"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="1、论文全名-—-Residual-Attention-Network-for-Image-Classification"><a href="#1、论文全名-—-Residual-Attention-Network-for-Image-Classification" class="headerlink" title="1、论文全名 — Residual Attention Network for Image Classification"></a>1、论文全名 — Residual Attention Network for Image Classification</h1><h1 id="2、-emsp-整体结构"><a href="#2、-emsp-整体结构" class="headerlink" title="2、&emsp;整体结构"></a>2、&emsp;整体结构</h1><p><img src="/2021/08/04/Residual%20Attention%20Network/1628056822799.png" alt="1628056822799"></p>
<p>左图:一个例子展示了特征和注意力面具之间的相互作用。</p>
<p>右图:举例说明在我们的网络中，不同的特征有不同的对应注意面具。</p>
<p><img src="/2021/08/04/Residual%20Attention%20Network/1628066487666.png" alt="1628066487666"></p>
<center>软掩码分支结构</center>
&emsp;&emsp;&emsp;掩码分支包含**快速前馈扫描**和**自上而下的反馈**步骤。**前者快速采集整幅图像的全局信息，后者将全局信息与原始特征图相结合**。在卷积神经网络中，这两个步骤展开为自下而上、自上而下的全卷积结构。从输入来看，在少量剩余单位后，**进行几次最大池化以快速增加感受野**。在达到最低分辨率后，全局信息通过**对称**的自上而下架构进行扩展，以指导每个位置的输入特征。**线性插值在一些残差单位后向上采样输出**。**双线性插值的数量与最大池化相同，以保持输出大小与输入要素图相同**。然后，连续两个1 × 1卷积层后，sigmoid层对输出进行归一化，范围为[0，1]。我们还在自下而上和自上而下的零件之间添加了跳跃连接，以从不同的比例捕获信息。

<p><img src="/2021/08/04/Residual%20Attention%20Network/1628065657226.png" alt="1628065657226"></p>
<center>残差注意力网络结构</center>
超参数p表示在分成主干分支和掩码分支**之前预处理剩余单元的数量**。t表示**主干分支**中剩余单元的数量。r表示**掩码分支**中相邻汇集层之间的剩余单元数。

<center>软掩码分支结构</center>
# 3、 残差注意力网络分析

<p>我们的剩余注意力网络是通过堆叠多个注意力模块构建的。每个注意模块分为两个分支:掩码分支和主干分支。主干分支执行特征处理，并且可以适应任何最先进的网络结构。</p>
<p>使用pre-activation Residual Unit、ResNeXt 和Inception 作为剩余注意网络的基本单元来构建注意模块</p>
<h2 id="3-1-emsp-注意力剩余学习"><a href="#3-1-emsp-注意力剩余学习" class="headerlink" title="3.1&emsp;注意力剩余学习"></a>3.1&emsp;注意力剩余学习</h2><p>给定输入为x的主干分支输出T(x)，掩码分支使用自下而上自上而下的结构来学习相同大小的掩码M(x)，该掩码对输出特征T(x)进行软加权。</p>
<p>注意模块H的输出为：     &emsp;&emsp;&emsp;&emsp;</p>
<img src="/2021/08/04/Residual%20Attention%20Network/1628066609969.png" alt="1628066609969">

<p>在Attention Modules 中，attention mask不仅可以在前向推理过程中充当特征选择器，还可以在反向传播过程中充当梯度更新过滤器。在软遮罩分支中，输入要素的遮罩梯度为:</p>
<p><img src="/2021/08/04/Residual%20Attention%20Network/1628129507416.png" alt="1628129507416"></p>
<p>θ是掩码分支参数，φ是主干分支参数。此属性使注意力模块对有噪声的标签具有鲁棒性。遮罩分支可以防止错误的渐变(来自有噪声的标签)来更新主干参数。</p>
<h2 id="3-2-emsp-软掩码分支（Soft-Mask-Branch）"><a href="#3-2-emsp-软掩码分支（Soft-Mask-Branch）" class="headerlink" title="3.2&emsp;软掩码分支（Soft Mask Branch）"></a>3.2&emsp;软掩码分支（Soft Mask Branch）</h2><p>掩码分支目的是改进主干分支特征，而不是直接解决复杂的问题</p>
<p><img src="/2021/08/04/Residual%20Attention%20Network/1628131500040.png" alt="1628131500040"></p>
<center>掩蔽分支和主干分支的感受野比较</center>
注意力模块的输出修改为：&emsp;&emsp;&emsp;

<p><img src="/2021/08/04/Residual%20Attention%20Network/1628130312831.png" alt="1628130312831"></p>
<p><img src="/2021/08/04/Residual%20Attention%20Network/1628130471469.png" alt="1628130471469" style="zoom:85%;">是由深度卷积网络生成的特征mask分支M(x)，它们充当特征选择器，增强好的特征并抑制来自主干特征的噪声。</p>
<h2 id="3-3-emsp-遮罩分支提供的注意力随着主干分支特征而自适应地改变"><a href="#3-3-emsp-遮罩分支提供的注意力随着主干分支特征而自适应地改变" class="headerlink" title="3.3&emsp;遮罩分支提供的注意力随着主干分支特征而自适应地改变"></a>3.3&emsp;遮罩分支提供的注意力随着主干分支特征而自适应地改变</h2><p><img src="/2021/08/04/Residual%20Attention%20Network/1628133676240.png" alt="1628133676240"></p>
<p>xi表示第I个空间位置的特征向量。I在所有空间位置范围内，c在所有通道范围内</p>
<p>无附加限制的混合注意f1对每个通道和空间位置使用简单的sigmoid。</p>
<p>通道注意力f2在所有通道内对每个空间位置执行L2归一化，以去除空间信息。</p>
<p>空间注意力f3在来自每个通道的特征图内执行标准化，然后sigmoid以获得仅与空间信息相关的软掩模。</p>
<h1 id="4、-emsp-总结"><a href="#4、-emsp-总结" class="headerlink" title="4、&emsp;总结"></a>4、&emsp;总结</h1><p>普通注意力模型的缺点：</p>
<p>1、<strong>具有杂乱背景、复杂场景和大的外观变化的图像需要通过不同类型的关注来建模</strong>。在这种情况下，来自不同层的特征需要由不同的注意力遮罩来建模。使用单个掩码分支将需要指数数量的通道来捕获不同因素的所有组合。</p>
<p>2、<strong>单个注意模块只修改一次特征。如果映像的某些部分修改失败，以下网络模块将没有第二次机会。</strong></p>
<p><strong>本文提出的残余注意力网络缓解了上述问题</strong>。在注意力模块中，每个主干分支都有自己的掩码分支来学习专门针对其特征的注意力。</p>
<p><strong>1.堆叠网络结构:我们的剩余注意力网络是通过堆叠多个注意力模块来构建的。不同类型的注意力能够在不同的注意力模块中被捕获。改变 f（x）即可</strong></p>
<p><strong>2.整个soft mask branch 才是重点，输出的M当做残差加一以后再去卷积主干的特征</strong></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/">注意力模型</a></div><div class="post_share"><div class="social-share" data-image="https://tva3.sinaimg.cn/large/ad66c4fbgy1g3ew33gjqij21hc0u07ap.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/14/%E4%BB%A3%E7%A0%81%E6%B5%81%E7%A8%8B%E8%A7%A3%E8%AF%BB/"><img class="prev-cover" src="https://images8.alphacoders.com/864/864900.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">解读第四阶段素描GAN代码</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/03/MUNIT/"><img class="next-cover" src="https://www.10wallpaper.com/wallpaper/1366x768/1712/Angry_flame_Son_Goku_Dragon_Ball_Super_4K_HD_1366x768.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">MUNIT</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/03/U-GAT-IT/" title="U-GAT-IT"><img class="cover" src="https://images4.alphacoders.com/678/678317.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-03</div><div class="title">U-GAT-IT</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E5%85%A8%E5%90%8D-%E2%80%94-Residual-Attention-Network-for-Image-Classification"><span class="toc-number">1.</span> <span class="toc-text">1、论文全名 — Residual Attention Network for Image Classification</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2%E3%80%81-emsp-%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">2、 整体结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-emsp-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%89%A9%E4%BD%99%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.1.</span> <span class="toc-text">3.1 注意力剩余学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-emsp-%E8%BD%AF%E6%8E%A9%E7%A0%81%E5%88%86%E6%94%AF%EF%BC%88Soft-Mask-Branch%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">3.2 软掩码分支（Soft Mask Branch）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-emsp-%E9%81%AE%E7%BD%A9%E5%88%86%E6%94%AF%E6%8F%90%E4%BE%9B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%9A%8F%E7%9D%80%E4%B8%BB%E5%B9%B2%E5%88%86%E6%94%AF%E7%89%B9%E5%BE%81%E8%80%8C%E8%87%AA%E9%80%82%E5%BA%94%E5%9C%B0%E6%94%B9%E5%8F%98"><span class="toc-number">2.3.</span> <span class="toc-text">3.3 遮罩分支提供的注意力随着主干分支特征而自适应地改变</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4%E3%80%81-emsp-%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">4、 总结</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://tva3.sinaimg.cn/large/ad66c4fbgy1g3ew33gjqij21hc0u07ap.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 By George</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">努力努力再努力!!!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>