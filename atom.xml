<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lllllcy</title>
  
  <subtitle>2020.7</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-08-03T11:15:35.808Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>George</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MUNIT</title>
    <link href="http://example.com/2021/08/03/MUNIT/"/>
    <id>http://example.com/2021/08/03/MUNIT/</id>
    <published>2021-08-03T08:45:44.000Z</published>
    <updated>2021-08-03T11:15:35.808Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、论文全名-—-Multimodal-Unsupervised-Image-to-Image-Translation"><a href="#1、论文全名-—-Multimodal-Unsupervised-Image-to-Image-Translation" class="headerlink" title="1、论文全名 — Multimodal Unsupervised Image-to-Image Translation"></a>1、论文全名 — Multimodal Unsupervised Image-to-Image Translation</h1><h1 id="emsp-emsp-多模态无监督图像到图像翻译"><a href="#emsp-emsp-多模态无监督图像到图像翻译" class="headerlink" title="&emsp;&emsp;(多模态无监督图像到图像翻译)"></a>&emsp;&emsp;(多模态无监督图像到图像翻译)</h1><h1 id="2、整体结构"><a href="#2、整体结构" class="headerlink" title="2、整体结构"></a>2、整体结构</h1><p><img src="/2021/08/03/MUNIT/1627986033107.png" alt="1627986033107"></p><p><img src="/2021/08/03/MUNIT/1627980504738.png" alt="1627980504738"></p><h1 id="3、过程"><a href="#3、过程" class="headerlink" title="3、过程"></a>3、过程</h1><p>&emsp;&emsp;如图1 (a)所示，我们的框架做了几个假设。我们首先假设图像的潜在空间可以分解为内容空间C和风格空间S。我们进一步假设不同领域的图像共享一个共同的内容空间，但不共享样式空间。为了将图像翻译到目标域，我们将其内容代码与目标样式空间中的随机样式代码重新组合(图1(b))。</p><p>&emsp;&emsp;内容代码对翻译过程中应该保留的信息进行编码，而样式代码表示输入图像中不包含的其余变体。通过采样不同的风格代码，我们的模型能够产生不同的多模态输出。</p><p><img src="/2021/08/03/MUNIT/1627981675082.png" alt="1627981675082"></p><p>&emsp;&emsp;如图2(a)所示，每个自动编码器的潜在代码被分解成内容代码ci和样式代码si。如图2(b)所示，通过交换编码器-解码器对来执行图像到图像的转换。</p><p>&emsp;&emsp;为了将图像x1∈x1翻译成X2，我们首先提取其内容潜在代码<img src="/2021/08/03/MUNIT/1627982033272.png" alt="1627982033272" style="zoom:60%;">，并从先验分布<img src="/2021/08/03/MUNIT/1627982067523.png" alt="1627982067523" style="zoom:50%;">中随机绘制一个风格潜在代码s2。然后，我们使用G2产生最终输出图像<img src="/2021/08/03/MUNIT/1627982112763.png" alt="1627982112763" style="zoom:60%;">。</p><h2 id="4、损失函数设计"><a href="#4、损失函数设计" class="headerlink" title="4、损失函数设计"></a>4、损失函数设计</h2><h2 id="4-1-emsp-双向重建损失"><a href="#4-1-emsp-双向重建损失" class="headerlink" title="4.1&emsp;双向重建损失"></a>4.1&emsp;双向重建损失</h2><p><strong>图像重建损失:</strong> </p><p><img src="/2021/08/03/MUNIT/1627986277230.png" alt="1627986277230"></p><p><strong>潜在的重建损失：</strong>（内容重建损失和风格重建损失）</p><p><img src="/2021/08/03/MUNIT/1627986328694.png" alt="1627986328694"></p><p>在给定不同风格代码的情况下，风格重建损失具有鼓励不同输出的效果，内容重构损失重新鼓励翻译图像以保留输入图像的语义内容。</p><h2 id="4-2-emsp-对抗损失"><a href="#4-2-emsp-对抗损失" class="headerlink" title="4.2&emsp;对抗损失"></a>4.2&emsp;对抗损失</h2><p>先验分布<img src="/2021/08/03/MUNIT/1627987418253.png" alt="1627987418253" style="zoom:50%;">，边缘分布p(x）</p><p><img src="/2021/08/03/MUNIT/1627986915469.png" alt="1627986915469"></p><h2 id="4-3-emsp-总损失"><a href="#4-3-emsp-总损失" class="headerlink" title="4.3&emsp;总损失"></a>4.3&emsp;总损失</h2><img src="/2021/08/03/MUNIT/1627987289574.png" alt="1627987289574" style="zoom:75%;"><img src="/2021/08/03/MUNIT/1627987296909.png" alt="1627987296909" style="zoom:75%;"><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><p>&emsp;&emsp;<strong>网络设计有点复杂，思路很容易理解。将图片分成风格编码器和内容编码器，然后拆开编码，最后再进行组合，同一个内容可以混合不同的风格。设计损失函数的时候也有重建风格损失，重建内容损失。</strong></p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1、论文全名-—-Multimodal-Unsupervised-Image-to-Image-Translation&quot;&gt;&lt;a href=&quot;#1、论文全名-—-Multimodal-Unsupervised-Image-to-Image-Translation&quot; </summary>
      
    
    
    
    <category term="论文阅读（第一层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    <category term="GAN（第二层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/GAN%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    
    <category term="GAN，论文阅读" scheme="http://example.com/tags/GAN%EF%BC%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>U-GAT-IT</title>
    <link href="http://example.com/2021/08/03/U-GAT-IT/"/>
    <id>http://example.com/2021/08/03/U-GAT-IT/</id>
    <published>2021-08-03T03:44:36.000Z</published>
    <updated>2021-08-03T08:02:06.383Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、论文全名-—-U-GAT-IT-UNSUPERVISED-GENERATIVE-ATTENTIONAL-NETWORKS-WITH-ADAPTIVE-LAYERINSTANCE-NORMALIZATION-FOR-IMAGE-TO-IMAGE"><a href="#1、论文全名-—-U-GAT-IT-UNSUPERVISED-GENERATIVE-ATTENTIONAL-NETWORKS-WITH-ADAPTIVE-LAYERINSTANCE-NORMALIZATION-FOR-IMAGE-TO-IMAGE" class="headerlink" title="1、论文全名 — U-GAT-IT: UNSUPERVISED GENERATIVE ATTENTIONAL NETWORKS  WITH ADAPTIVE LAYERINSTANCE NORMALIZATION FOR IMAGE-TO-IMAGE"></a>1、论文全名 — U-GAT-IT: UNSUPERVISED GENERATIVE ATTENTIONAL NETWORKS  WITH ADAPTIVE LAYERINSTANCE NORMALIZATION FOR IMAGE-TO-IMAGE</h1><p>TRANSLATION（用于图像到图像翻译的具有自适应层度归一化的无监督生成注意网络）</p><h1 id="2、-emsp-整体结构"><a href="#2、-emsp-整体结构" class="headerlink" title="2、&emsp;整体结构"></a>2、&emsp;整体结构</h1><p><img src="/2021/08/03/U-GAT-IT/1627963121891.png" alt="1627963121891"></p><p>​                                <center><strong>U - GAT - IT结构图</strong></center></p><h1 id="2、-emsp-U-GAT-IT过程"><a href="#2、-emsp-U-GAT-IT过程" class="headerlink" title="2、&emsp;U - GAT - IT过程"></a>2、&emsp;U - GAT - IT过程</h1><p>&emsp;&emsp;目标是训练一个函数G_(s→t)，该函数仅使用从每个域提取的不成对样本将图像从源域Xs映射到目标域Xt。我们的框架由两个生成器G_(s→t)和G_(t→s)和两个鉴别器Ds 和 Dt 组成。我们将注意力模块集成到生成器和鉴别器中。<strong>鉴别器中的注意力模块引导生成器聚焦于对生成逼真图像至关重要的区域。生成器中的注意模块关注与其他域不同的区域。</strong></p><h2 id="2-1-生成器G-s→t"><a href="#2-1-生成器G-s→t" class="headerlink" title="2.1  生成器G_(s→t)"></a>2.1  生成器G_(s→t)</h2><p>组成成分：编码器Es，解码器Gt，辅助分类器 ηs，其中ηs(x)表示x来自Xs的概率，</p><p>通过使用全局平均池和全局最大池，<strong>辅助分类器被训练来学习源域的第k特征图的权重w^k_(s),</strong></p><p><img src="/2021/08/03/U-GAT-IT/1627965570665.png" alt="1627965570665"></p><p>利用w^k_(s)，们可以计算一组域特定注意特征图a_s(x)为：&emsp;&emsp;其中n为特征图的数量</p><img src="/2021/08/03/U-GAT-IT/1627965767793.png" alt="1627965767793" style="zoom:70%;"><p>然后，我们的翻译模型Gs→t变得等于G_t(a_s(x))。</p><p>**自适应层重要性归一化(AdaLIN)**，其参数在训练期间通过自适应地选择实例归一化(IN)和层归一化(LN)之间的适当比率从数据集学习。</p><p><img src="/2021/08/03/U-GAT-IT/1627965972979.png" alt="1627965972979"></p><p> µ_I,  µ_L, σ_I, σ_L分别是通道级均值、层级均值和标准差，γ和β是全连通层生成的参数，τ是学习率，△ρ表示优化器确定的参数更新向量(如梯度)。</p><p>LN 并不假设通道之间不相关，有时它不能很好地保持原始领域的内容结构，因为它只考虑了特征地图的全局统计。归一化技术AdaLIN通过选择性地保留或改变内容信息，结合了AdaIN和LN的优点，这有助于解决广泛的图像到图像翻译问题</p><h2 id="2-2-emsp-判别器Dt"><a href="#2-2-emsp-判别器Dt" class="headerlink" title="2.2 &emsp;判别器Dt"></a>2.2 &emsp;判别器Dt</h2><p>组成成分：是由编码器E_Dt、分类器C__Dt和辅助分类器  η_(Dt) 组成的多尺度模型。</p><p>x∞{  Xt，Gs→t(Xs) } 表示来自目标域和翻译后的源域的样本。与其他翻译模型不同，η_Dt(x)和D_t(x)都被训练来区分x是否来自 Xt 还是 Gs→t(Xs)。</p><p>给定一个样本x，Dt(x)使用编码特征图E_Dt(x)上面的权重w_Dt去寻找注意力特征图<img src="/2021/08/03/U-GAT-IT/1627973410325.png" alt="1627973410325" style="zoom:50%;">，这个编码特征图E_dT(x)由辅助分类器 ηDt(x)训练得来。然后，我们的鉴别器Dt(x)变得等于<img src="/2021/08/03/U-GAT-IT/1627973368892.png" alt="1627973368892" style="zoom:55%;"></p><h1 id="3、损失函数的设计"><a href="#3、损失函数的设计" class="headerlink" title="3、损失函数的设计"></a>3、损失函数的设计</h1><p>我们模型的全部目标包括四个损失函数。</p><p>这里，我们使用最小二乘GAN  (LSGAN)目标进行稳定训练，而不是使用普通的GAN目标。</p><h3 id="3-1-emsp-知识介绍-–-LSGAN"><a href="#3-1-emsp-知识介绍-–-LSGAN" class="headerlink" title="3.1 &emsp;知识介绍 – LSGAN"></a>3.1 &emsp;知识介绍 – LSGAN</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Least Squares GAN   这篇文章针对的是原始GAN生成的图片质量不高以及训练过程不稳定这两个缺陷进行改进。</span><br><span class="line"></span><br><span class="line">改进方法就是将GAN的目标函数由交叉熵损失换成最小二乘损失，而且这一个改变同时解决了两个缺陷。</span><br><span class="line">为以交叉熵作为损失，会使得生成器不会再优化那些被判别器识别为真实图片的生成图片，即使这些生成图片距离判别器的决策边界仍然很远，也就是距真实数据比较远。因为它已经成功欺骗了判别器，这导致了生成器的生成图片质量并不高。</span><br><span class="line"></span><br><span class="line">最小二乘损失函数会对处于判别成真的那些远离决策边界的样本进行惩罚，把远离决策边界的假样本拖进决策边界，从而提高生成图片的质量。</span><br></pre></td></tr></table></figure><p><img src="/2021/08/03/U-GAT-IT/1627975227737.png" alt="1627975227737"></p><p><img src="/2021/08/03/U-GAT-IT/1627975341016.png" alt="1627975341016"></p><h3 id="3-2-emsp-对抗损失"><a href="#3-2-emsp-对抗损失" class="headerlink" title="3.2&emsp;对抗损失"></a>3.2&emsp;对抗损失</h3><p><img src="/2021/08/03/U-GAT-IT/1627975505284.png" alt="1627975505284"></p><h3 id="3-3-emsp-循环损失"><a href="#3-3-emsp-循环损失" class="headerlink" title="3.3&emsp;循环损失"></a>3.3&emsp;循环损失</h3><h3 id="emsp-emsp-emsp-可以缓解模式崩溃问题"><a href="#emsp-emsp-emsp-可以缓解模式崩溃问题" class="headerlink" title="&emsp;&emsp;&emsp;可以缓解模式崩溃问题"></a>&emsp;&emsp;&emsp;可以缓解模式崩溃问题</h3><p><img src="/2021/08/03/U-GAT-IT/1627975587820.png" alt="1627975587820"></p><h3 id="3-4-emsp-身份损失"><a href="#3-4-emsp-身份损失" class="headerlink" title="3.4&emsp;身份损失"></a>3.4&emsp;身份损失</h3><h3 id="emsp-emsp-emsp-为了确保输入图像和输出图像的颜色分布相似"><a href="#emsp-emsp-emsp-为了确保输入图像和输出图像的颜色分布相似" class="headerlink" title="&emsp;&emsp;&emsp;为了确保输入图像和输出图像的颜色分布相似"></a>&emsp;&emsp;&emsp;为了确保输入图像和输出图像的颜色分布相似</h3><p><img src="/2021/08/03/U-GAT-IT/1627975653043.png" alt="1627975653043"></p><h3 id="3-5-emsp-CAM损失"><a href="#3-5-emsp-CAM损失" class="headerlink" title="3.5&emsp;CAM损失"></a>3.5&emsp;CAM损失</h3><h3 id="emsp-emsp-emsp-通过利用来自辅助分类器ηs和η-Dt的信息-是的G-s→t-和Dt了解他们需要改进的地方，或者在当前状态下两个领域的最大区别"><a href="#emsp-emsp-emsp-通过利用来自辅助分类器ηs和η-Dt的信息-是的G-s→t-和Dt了解他们需要改进的地方，或者在当前状态下两个领域的最大区别" class="headerlink" title="&emsp;&emsp;&emsp;通过利用来自辅助分类器ηs和η_Dt的信息,是的G_(s→t)和Dt了解他们需要改进的地方，或者在当前状态下两个领域的最大区别:"></a>&emsp;&emsp;&emsp;通过利用来自辅助分类器ηs和η_Dt的信息,是的G_(s→t)和Dt了解他们需要改进的地方，或者在当前状态下两个领域的最大区别:</h3><p><img src="/2021/08/03/U-GAT-IT/1627975912525.png" alt="1627975912525"></p><h3 id="3-6-emsp-总损失"><a href="#3-6-emsp-总损失" class="headerlink" title="3.6 &emsp;总损失"></a>3.6 &emsp;总损失</h3><p><img src="/2021/08/03/U-GAT-IT/1627976087788.png" alt="1627976087788"></p><hr><h1 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h1><h2 id="emsp-emsp-个人理解创新的地方在于"><a href="#emsp-emsp-个人理解创新的地方在于" class="headerlink" title="&emsp;&emsp;个人理解创新的地方在于"></a>&emsp;&emsp;<strong>个人理解创新的地方在于</strong></h2><ol><li><p><strong>CAM损失，能够帮助两个不同域直接和合成。</strong></p></li><li><p><strong>有两种不同的特征图——编码器特征图和注意力特征图。</strong></p><p><strong>Dt(x)使用编码特征图E_Dt(x)上面的权重w_Dt去寻找注意力特征图<img src="/2021/08/03/U-GAT-IT/1627973410325.png" alt="1627973410325" style="zoom:50%;">，这个编码特征图E_dT(x)由辅助分类器 ηDt(x)训练得来</strong></p></li><li><p><strong>鉴别器中的注意力模块引导生成器聚焦于对生成逼真图像至关重要的区域。生成器中的注意模块关注与其他域不同的区域。</strong></p></li><li><p><strong>损失函数使用的是LSGAN)目标进行稳定训练，而不是传统GAN</strong></p></li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1、论文全名-—-U-GAT-IT-UNSUPERVISED-GENERATIVE-ATTENTIONAL-NETWORKS-WITH-ADAPTIVE-LAYERINSTANCE-NORMALIZATION-FOR-IMAGE-TO-IMAGE&quot;&gt;&lt;a href</summary>
      
    
    
    
    <category term="论文阅读（第一层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    <category term="GAN（第二层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/GAN%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    
    <category term="无监督注意力模型" scheme="http://example.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Stack++</title>
    <link href="http://example.com/2021/08/01/StackGAN-v2/"/>
    <id>http://example.com/2021/08/01/StackGAN-v2/</id>
    <published>2021-08-01T13:18:44.000Z</published>
    <updated>2021-08-03T11:15:38.900Z</updated>
    
    <content type="html"><![CDATA[  <meta name="referrer" content="no-referrer">  # 1、论文全名 --- stackGAN++: Realistic Image Synthesis with    Stacked Generative Adversarial Networks<h1 id="2、整体结构"><a href="#2、整体结构" class="headerlink" title="2、整体结构"></a>2、整体结构</h1><p><img src="/2021/08/01/StackGAN-v2/1627825385517.png"></p><p>​                                      <center>StackGAN-v2流程图</center></p><h1 id="3、过程以及损失函数的设计"><a href="#3、过程以及损失函数的设计" class="headerlink" title="3、过程以及损失函数的设计"></a>3、过程以及损失函数的设计</h1><p> &emsp; &emsp;StackGAN-v2由多个生成器(G)和鉴别器(D)组成，它们在树状结构中共享大多数参数。网络的输入可以被视为树的根，并且多尺度图像从树的不同分支生成。从低分辨率到高分辨率的图像是从树的不同分支生成的。</p><p>&emsp;&emsp;在每个分支，生成器捕获该比例下的图像分布，鉴别器估计样本来自该比例的训练图像而不是生成器的概率。生成器被联合训练以逼近多个分布，并且生成器和鉴别器以交替的方式被训练。在本节中，我们探讨了两种类型的多重分布:(1)多尺度图像分布以及(2)联合条件和无条件图像分布。</p><h3 id="3-1-多尺度图像分布近似-Multi-scale-image-distributions-approximation"><a href="#3-1-多尺度图像分布近似-Multi-scale-image-distributions-approximation" class="headerlink" title="3.1 多尺度图像分布近似(Multi-scale image distributions approximation)"></a>3.1 多尺度图像分布近似(Multi-scale image distributions approximation)</h3><p>&emsp;&emsp; StackGAN-v2以一个噪声向量z作为输入，并有多个生成器来生成不同比例的图像。噪声是一种先验分布，通常为标准正态分布。潜在变量z被逐层转换为隐藏特征。我们通过非线性变换计算每个发生器G的隐藏特征hi。</p><p>&emsp;&emsp;hi表示第i个分支的隐藏特征，m为总分支数，为了捕捉在前面的分支中省略的信息，噪声矢量z连接到隐藏特征h_(i -1)，作为 Fi 的输入去计算hi。</p><p><img src="/2021/08/01/StackGAN-v2/1627828037446-1627873594719.png"></p><p><img src="/2021/08/01/StackGAN-v2/1627828068601.png"></p><p>&emsp;&emsp;在每个生成器Gi之后跟着一个鉴别器Di，通过最小化以下交叉熵损失来训练鉴别器Di，该鉴别器Di以真实样本xi和伪样本si作为输入，以将输入分类为两类(真或假)。</p><p><img src="/2021/08/01/StackGAN-v2/1627828305495.png"></p><p>&emsp;&emsp;多个鉴别器被并行训练，并且每个鉴别器聚焦在单个图像尺度上。</p><p>&emsp;&emsp;在训练好的鉴别器的指导下，生成器通过最小化下面的损失函数被优化以联合逼近多尺度图像分布</p><p><img src="/2021/08/01/StackGAN-v2/1627828424158.png"></p><p>&emsp;其中l_g是用于近似第i个尺度上的图像分布的损失函数。在训练过程中，鉴别器和生成器交替优化，直到收敛。</p><p>&emsp;&emsp;<strong>提出的StackGAN-v2的动机是，通过在多个尺度上建模数据分布，如果这些模型分布中的任何一个与该尺度上的真实数据分布共享支持，则重叠可以提供良好的梯度信号，以在多个尺度上加速或稳定整个网络的训练。</strong></p><h3 id="3-2-联合条件和无条件图像分布近似（-JCD-）"><a href="#3-2-联合条件和无条件图像分布近似（-JCD-）" class="headerlink" title="3.2 联合条件和无条件图像分布近似（ JCD ）"></a>3.2 联合条件和无条件图像分布近似（ JCD ）</h3><p>&emsp;&emsp;<strong>无条件损失决定图像的真假，有条件损失决定图像和条件是否匹配。</strong>    </p><p>&emsp;&emsp;对于我们的条件StackGAN-v2的G，F0和Fi 被转换为将条件向量c作为输入。对于Fi，调节向量c代替噪声向量z，以鼓励生成器根据调节变量绘制具有更多细节的图像。因此，多尺度样本现在由si=  Gi(hi)生成。</p><p><img src="/2021/08/01/StackGAN-v2/1627870863531.png" alt="1627870863531"></p><p><img src="/2021/08/01/StackGAN-v2/1627870873418.png" alt="1627870873418"></p><p>&emsp;&emsp;<strong>训练条件D的目标函数现在由两个项组成，无条件损失和条件损失。</strong></p><p><img src="/2021/08/01/StackGAN-v2/1627870996625.png" alt="1627870996625"></p><p>&emsp;&emsp;<strong>这部分的损失函数为</strong></p><p><img src="/2021/08/01/StackGAN-v2/1627871482209.png" alt="1627871482209"></p><h3 id="3-3-颜色一致性正则化（-Color-consistency-regularization）"><a href="#3-3-颜色一致性正则化（-Color-consistency-regularization）" class="headerlink" title="3.3 颜色一致性正则化（ Color-consistency regularization）"></a>3.3 颜色一致性正则化（ Color-consistency regularization）</h3><p>&emsp;&emsp;当我们在不同的生成器上增加图像分辨率时，在不同比例下生成的图像应该共享相似的基本结构和颜色。引入颜色一致性正则化项，以保持从不同G的相同输入生成的样本在颜色上更加一致，从而提高生成图像的质量。</p><p><img src="/2021/08/01/StackGAN-v2/1627872407715.png" alt="1627872407715"></p><p><img src="/2021/08/01/StackGAN-v2/1627872459349.png" alt="1627872459349"></p><p><img src="/2021/08/01/StackGAN-v2/1627873918668.png" alt="1627873918668"></p><p>X_k表示生成的图像中的一个像素，下面的代表给定图像的像素的平均值和协方差，N为像素个数。</p><p><strong>颜色一致性正则化的目的是最小化不同尺度之间的像素平均值和协方差的差异，以促进一致性。</strong></p><p><img src="/2021/08/01/StackGAN-v2/1627872829324.png" alt="1627872829324"></p><hr><h1 id="4、总结："><a href="#4、总结：" class="headerlink" title="4、总结："></a>4、总结：</h1><h3 id="emsp-emsp-这个文章是在StackGAN上面的延续，针对条件生成任务和无条件生成任务，提出了一种先进的多阶段生成对抗网络体系结构StackGAN-v2。"><a href="#emsp-emsp-这个文章是在StackGAN上面的延续，针对条件生成任务和无条件生成任务，提出了一种先进的多阶段生成对抗网络体系结构StackGAN-v2。" class="headerlink" title="&emsp;&emsp;这个文章是在StackGAN上面的延续，针对条件生成任务和无条件生成任务，提出了一种先进的多阶段生成对抗网络体系结构StackGAN-v2。"></a>&emsp;&emsp;这个文章是在StackGAN上面的延续，针对条件生成任务和无条件生成任务，提出了一种先进的多阶段生成对抗网络体系结构StackGAN-v2。</h3><h2 id="emsp-emsp-stackGAN有三个主要贡献："><a href="#emsp-emsp-stackGAN有三个主要贡献：" class="headerlink" title="&emsp;&emsp;stackGAN有三个主要贡献："></a>&emsp;&emsp;stackGAN有三个主要贡献：</h2><ol><li><strong>StackGAN-v1首次从文本描述中分阶段生成具有照片级逼真细节的256×256分辨率的图像。</strong></li><li><strong>StackGAN-v2通过联合逼近多个分布，进一步提高了生成图像的质量，稳定了GANs的训练。</strong></li><li><strong>提出了一个颜色一致性正则化项来指导我们的生成器在不同的尺度上生成更多的相干样本。</strong></li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">  &lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;  
# 1、论文全名 --- stackGAN++: Realistic Image Synthesis with    Stacked Generative Adversarial N</summary>
      
    
    
    
    <category term="论文阅读（第一层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    <category term="GAN（第二层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/GAN%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    
    <category term="GAN，论文阅读" scheme="http://example.com/tags/GAN%EF%BC%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>StackGAN</title>
    <link href="http://example.com/2021/08/01/StackGAN/"/>
    <id>http://example.com/2021/08/01/StackGAN/</id>
    <published>2021-08-01T11:20:10.000Z</published>
    <updated>2021-08-03T08:03:55.168Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、论文全名-—-StackGAN-Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks"><a href="#1、论文全名-—-StackGAN-Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks" class="headerlink" title="1、论文全名 — StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks"></a>1、论文全名 — StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</h1><h1 id="2、整体结构"><a href="#2、整体结构" class="headerlink" title="2、整体结构"></a>2、整体结构</h1><p><img src="/2021/08/01/StackGAN/1627817144163-1627822726262.png"></p><p>​                                                                 <center> <strong>StackGAN 的流程图</strong></center> </p><h1 id="3、-StackGAN过程"><a href="#3、-StackGAN过程" class="headerlink" title="3、 StackGAN过程"></a>3、 StackGAN过程</h1><p>&emsp;&emsp;第一阶段生成器通过从给定文本中绘制对象的粗略形状和基本颜色，并从随机噪声向量中绘制背景，来绘制低分辨率图像。</p><p>&emsp;&emsp; 第二阶段生成器纠正缺陷，并在第一阶段的结果中添加引人注目的细节，产生更真实的高分辨率图像。</p><h2 id="3-1-第一阶段（并非自己关注的重点）"><a href="#3-1-第一阶段（并非自己关注的重点）" class="headerlink" title="3.1  第一阶段（并非自己关注的重点）"></a>3.1  第一阶段（并非自己关注的重点）</h2><p>&emsp;&emsp;ϕt 表示给定描述的文本嵌入，它是由本文中预先训练的编码器生成的。条件^C0，随机变量Z。</p><p>&emsp;&emsp;<em>第一阶段随机交替的最大化等式 L(Do)和最小化 L(Go)来训练生成器Go和判别器Do</em></p><img src="/2021/08/01/StackGAN/1627817709932-1627822752622.png" style="zoom:67%;"><img src="/2021/08/01/StackGAN/1627817814686-1627822752622.png" alt="1627817814686" style="zoom:67%;"><h2 id="3-2-第二阶段（重点关注第一阶段过渡到第二阶段的代码）"><a href="#3-2-第二阶段（重点关注第一阶段过渡到第二阶段的代码）" class="headerlink" title="3.2 第二阶段（重点关注第一阶段过渡到第二阶段的代码）"></a>3.2 第二阶段（重点关注第一阶段过渡到第二阶段的代码）</h2><p>&emsp;&emsp; 第一阶段GAN生成的低分辨率图像通常缺少生动的物体部分，并且可能包含形状失真。文本中的一些细节也可能在第一阶段被省略，这对于生成照片真实感图像至关重要。第二阶段GAN是建立在第一阶段结果的基础上，以生成高分辨率图像。它以低分辨率图像和再次嵌入文本为条件，以纠正第一阶段结果中的缺陷。</p><p>&emsp;&emsp;第二阶段不再使用随机噪声，使用的so表示第一阶段生成结果。</p><p>&emsp;&emsp;<em>第二阶段随机交替的最大化等式 L(D)和最小化 L(G)来训练生成器G和判别器D</em></p><img src="../images/StackGAN/1627818853115.png" alt="1627818853115" style="zoom:67%;"><img src="/2021/08/01/StackGAN/1627818853115-1627822752622.png" alt="1627818853115" style="zoom:67%;"><h1 id="3、总结"><a href="#3、总结" class="headerlink" title="3、总结"></a>3、总结</h1><h4 id="emsp-emsp-这篇文章是利用文本生成来生成图像，所以很多的细节不需要掌握、重点学习的是怎么把前面一个阶段的图放入下一个阶段。看论文只是次要、还是得复现代码。"><a href="#emsp-emsp-这篇文章是利用文本生成来生成图像，所以很多的细节不需要掌握、重点学习的是怎么把前面一个阶段的图放入下一个阶段。看论文只是次要、还是得复现代码。" class="headerlink" title="&emsp;&emsp;这篇文章是利用文本生成来生成图像，所以很多的细节不需要掌握、重点学习的是怎么把前面一个阶段的图放入下一个阶段。看论文只是次要、还是得复现代码。"></a>&emsp;&emsp;这篇文章是利用文本生成来生成图像，所以很多的细节不需要掌握、重点学习的是怎么把前面一个阶段的图放入下一个阶段。看论文只是次要、还是得复现代码。</h4><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1、论文全名-—-StackGAN-Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks&quot;&gt;&lt;a href=&quot;#1、论文全名-—-StackGAN-</summary>
      
    
    
    
    <category term="论文阅读（第一层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    <category term="GAN（第二层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/GAN%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    
    <category term="GAN，论文阅读" scheme="http://example.com/tags/GAN%EF%BC%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Unsupervised Attention-guided GAN</title>
    <link href="http://example.com/2021/08/01/Unsupervised-Attention-guided/"/>
    <id>http://example.com/2021/08/01/Unsupervised-Attention-guided/</id>
    <published>2021-08-01T11:20:10.000Z</published>
    <updated>2021-08-03T11:10:39.450Z</updated>
    
    <content type="html"><![CDATA[  <meta name="referrer" content="no-referrer">  # 1、论文全名 --- Unsupervised Attention-guided Image-to-Image Translation<h1 id="2、-emsp-整体结构"><a href="#2、-emsp-整体结构" class="headerlink" title="2、&emsp;整体结构"></a>2、&emsp;整体结构</h1><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627892637831.png" alt="1627892637831"></p><p>​                                                          <center>UAG-GAN流程图</center></p><h1 id="3、-emsp-UAG-GAN过程"><a href="#3、-emsp-UAG-GAN过程" class="headerlink" title="3、&emsp;UAG-GAN过程"></a>3、&emsp;UAG-GAN过程</h1><p>&emsp;&emsp;&emsp;文章出发点是在cycleGAN 的基础上改进的，图像风格变换实质上是两个不同域之间的相互转换。</p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627893141106.png" alt="1627893141106"></p><p>&emsp;&emsp;为了进一步处理，需要解决两个主要任务：</p><ol><li>在每个图像中定位要平移的区域，</li><li>对定位的区域应用正确的平移。</li></ol><h2 id="3-1-emsp-生成器G解读"><a href="#3-1-emsp-生成器G解读" class="headerlink" title="3.1 &emsp;生成器G解读"></a>3.1 &emsp;生成器G解读</h2><p>&emsp;&emsp;A_S与A_T表示两个不同的注意力网络，Sa 和 Ta分别是由 S 和 T 导出的注意图。每个注意力图包含每像素[0，1]的估计值。<strong>如果注意力图所有值为0，则输出背景项；如果全为1，则输出原图。</strong>将输入图像输入到生成器后，我们使用逐元素乘积将学习到的掩码应用到生成的图像，然后使用应用于输入图像的反遮罩添加背景。因此，A_S 和 A_T 与G一起训练，如图所示。</p><p>&emsp;&emsp;首先，我们将输入图像 s∈S 馈送到生成器F_(S→T), 生成器 F_(S→T)将s映射到目标域T，然后，将相同的输入馈送到注意力网络A_S，得到注意力图 Sa 。为了创建“前景”对象sf，我们把每个RGB通道上的注意力模型 Sa 和生成器F_(S→T)进行逐元素乘积。然后，对注意力图Sa取反，再和原图S进行逐元素成绩，得到“背景”图像Sb。最后，将前景和后景图相加即可得到最终图像。</p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627894572673.png" alt="1627894572673"></p><h2 id="3-2-emsp-损失函数"><a href="#3-2-emsp-损失函数" class="headerlink" title="3.2 &emsp;损失函数"></a>3.2 &emsp;损失函数</h2><p>&emsp;<strong>对抗损失：</strong></p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627895057172.png" alt="1627895057172"></p><p>&emsp;</p><p>&emsp;<strong>循环一致性损失：</strong>   &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;其中 S‘’ 是生成的T域图片再转回S域的图像。</p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627895140042.png" alt="1627895140042"></p><p>&emsp;<strong>最终损失：</strong></p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627901641560.png" alt="1627901641560"></p><p>&emsp;<strong>在整个实验中，我们通过求解极小极大优化问题得到L的最优参数:</strong></p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627901652097.png" alt="1627901652097"></p><h2 id="3-3-emsp-判别器D解读"><a href="#3-3-emsp-判别器D解读" class="headerlink" title="3.3 &emsp;判别器D解读"></a>3.3 &emsp;判别器D解读</h2><p>&emsp;&emsp;生成器仅作用于关注区域: 随着注意力网络训练在寻找前景方面会变得更加准确。但是也有两个引起误差的行为：（1）注意力图慢慢的包括越来越多的背景，向完全关注的图收敛 (<strong>图中的所有值都收敛到1</strong>)。（2）生成器F_(S→T)将背景直接“绘制”到关注区域中.</p><p>&emsp;单纯用原图像S和通过<strong>已经训练好了的A_S生成的</strong>注意力图像Sa进行逐元素乘积是有问题的，因为反馈给鉴别器的真实样本现在依赖于最初未训练的注意力图sa。如果GAN中的所有网络都被联合训练，这将导致模式崩溃。为了克服这个问题，我们首先在30个时期的完整图像上训练辨别器，然后在注意力网络A_S和A_T发展起来后，切换到掩蔽图像。</p><p>图像转换的流程如图：</p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627896930053.png" alt="1627896930053"></p><p>我们设置注意力取值不能为0，因为全0就变成了输出背景，故鉴别器设定学习注意图的阈值如下:</p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627901667458.png" alt="1627901667458"></p><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627901532979.png" alt="1627901532979"></p><h2 id="3-3-总结学习FS→T的训练程序"><a href="#3-3-总结学习FS→T的训练程序" class="headerlink" title="3.3 总结学习FS→T的训练程序"></a>3.3 总结学习FS→T的训练程序</h2><p><img src="/2021/08/01/Unsupervised-Attention-guided/1627901540811.png" alt="1627901540811"></p><h1 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h1><p><strong>&emsp;&emsp;这篇文章引入了注意力、把一张图片的合成分成了前景图片和后景图片的合成、分别处理后再相加。这样可以使得更加专注于前景图，对需要转化部分图像的处理达到更好的效果。</strong></p><p><strong>&emsp;&emsp;有一个想法：直接用前景减去后景是不是能够让素描的合成更加准确。</strong></p><hr>]]></content>
    
    
      
      
    <summary type="html">  &lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;  
# 1、论文全名 --- Unsupervised Attention-guided Image-to-Image Translation

&lt;h1 id=&quot;2、-emsp-整体结构</summary>
      
    
    
    
    <category term="论文阅读（第一层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    <category term="GAN（第二层级" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/GAN%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7/"/>
    
    
    <category term="GAN，论文阅读" scheme="http://example.com/tags/GAN%EF%BC%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>SGAN1</title>
    <link href="http://example.com/2021/08/01/SGAN1/"/>
    <id>http://example.com/2021/08/01/SGAN1/</id>
    <published>2021-08-01T08:25:02.000Z</published>
    <updated>2021-08-03T11:05:07.066Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、论文全名-—-Stacked-Generative-Adversarial-Networks"><a href="#1、论文全名-—-Stacked-Generative-Adversarial-Networks" class="headerlink" title="1、论文全名 — Stacked Generative Adversarial Networks"></a>1、论文全名 — Stacked Generative Adversarial Networks</h1><h1 id="2、整体结构"><a href="#2、整体结构" class="headerlink" title="2、整体结构"></a>2、整体结构</h1><p><img src="/2021/08/01/SGAN1/1627801448989-1627822969518.png"></p><p>​                                                                         <center> <strong>SGAN 的流程图</strong></center> </p><h1 id="3、SGAN过程"><a href="#3、SGAN过程" class="headerlink" title="3、SGAN过程"></a>3、SGAN过程</h1><ol><li><p>我们首先考虑一个为分类而预先训练的自下而上的DNN，它在全文中被称为编码器E。</p><p>我们定义了一堆自下而上的确定性非线性映射:， h0= x是输入图像。</p><p><img src="/2021/08/01/SGAN1/1627801876266-1627822969518.png" alt="1627801876266"></p></li><li><p>.提供一个预先训练好的编码器E，我们的目标是训练一个自顶向下的发生器G，它将E反转。具体来说，G由一个自顶向下的发生器Gi栈组成，每个G都被训练成反转一个自底向上的映射Ei。每个Gi将一个高级特征和一个噪声向量zi作为输入，并输出低级特征 ^hi。我们首先独立训练每个GAN，然后以端到端的方式联合训练它们。</p></li><li><p>每个G在独立训练阶段从编码器接收条件输入，在联合训练阶段从上层生成器接收条件输入。</p><p><img src="/2021/08/01/SGAN1/1627802710910-1627822969518.png" alt="1627802710910"></p></li></ol><p>&emsp;</p><p>&emsp;&emsp;<strong>图像的总变化可以被分解成多个级别，具有较高级别的语义变化(例如，属性、对象类别、粗略形状)和较低级别的变化(例如，详细的轮廓和纹理、背景杂波)。</strong></p><p><strong>&emsp;&emsp;为了采样图像，所有的 G 以自上而下的方式堆叠在一起，如图1 (c)所示。</strong></p><h1 id="4、损失函数的设计"><a href="#4、损失函数的设计" class="headerlink" title="4、损失函数的设计"></a>4、损失函数的设计</h1><p> &emsp;&emsp;<em>每个生成器G都是使用三个损失项的线性组合进行训练: 对抗性损失(adversarial loss)、</em></p><p><em>条件性损失(conditional loss)和熵损失(entropy loss)</em></p><p><img src="/2021/08/01/SGAN1/1627803164885-1627822969518.png" alt="1627803164885"></p><h2 id="4-1-对抗性损失-adversarial-loss"><a href="#4-1-对抗性损失-adversarial-loss" class="headerlink" title="4.1 对抗性损失(adversarial loss)"></a>4.1 对抗性损失(adversarial loss)</h2><p> 公式为：</p><p><img src="/2021/08/01/SGAN1/1627803661221-1627822969518.png" alt="1627803661221"></p><h2 id="4-2-条件性损失-conditional-loss"><a href="#4-2-条件性损失-conditional-loss" class="headerlink" title="4.2 条件性损失(conditional loss)"></a>4.2 条件性损失(conditional loss)</h2><p>&emsp;&emsp;&emsp;在每个stack中，一个生成器G被训练来捕获低级表示hi的分布，条件是高级表示hi+1。然而，在上面的公式中，生成器可能会选择忽略hi+1，并从零开始生成可能的^hi+1。本文使用的解决办法是添加一个条件损失项来正则化生成器将生成的较低级别表示 ^hi = Gi(hi+1，zi)反馈给编码器E，并计算恢复的较高级别表示。</p><p>&emsp;&emsp; 其中 f 是距离度量，表示标签和交叉熵的欧氏距离</p><p><img src="/2021/08/01/SGAN1/1627803944276-1627822969518.png" alt="1627803944276"></p><h2 id="4-3-熵损失-entropy-loss"><a href="#4-3-熵损失-entropy-loss" class="headerlink" title="4.3 熵损失(entropy loss)"></a>4.3 熵损失(entropy loss)</h2><p>&emsp;&emsp;简单地加上条件损失会导致另一个问题:生成器Gi学会了忽略噪声zi，并从hi+1中不确定地计算^hi。为了解决这个问题，我们希望当条件为hi+1时，生成的表示^hi足够多样，即条件熵H( ^hi | hi+1)应该尽可能高。由于直接最大化H( ^hi | hi+1)是难以处理的，我们建议改为最大化条件熵的变分下界。</p><p><strong>Variational Conditional Entropy Maximization（变分条件熵最大化）</strong>:</p><p>我们使用辅助分布Qi(zi |^hi)来近似真实后验Pi( zi |^hi)，并且用称为熵损失的损失项来扩充训练目标:</p><p><img src="/2021/08/01/SGAN1/1627805251705-1627822969519.png" alt="1627805251705"></p><p><strong>最小化的L（ent）Gi 等价于最大化的H（^hi 丨hi+1），证明省略。</strong></p><h1 id="5、总结："><a href="#5、总结：" class="headerlink" title="5、总结："></a>5、总结：</h1><h4 id="emsp-emsp-这篇文章提出的新模型训练的生成器G刚好是编码器E的反过来，整个模型是由自上而下的GAN组合而成，每个GAN都生成了表示高级表示的低级特征。同时引入了一个条件损失，它鼓励使用来自上面一层的条件信息，以及一个新的熵损失，他可以最大化G输出的条件熵损失。"><a href="#emsp-emsp-这篇文章提出的新模型训练的生成器G刚好是编码器E的反过来，整个模型是由自上而下的GAN组合而成，每个GAN都生成了表示高级表示的低级特征。同时引入了一个条件损失，它鼓励使用来自上面一层的条件信息，以及一个新的熵损失，他可以最大化G输出的条件熵损失。" class="headerlink" title="&emsp;&emsp;这篇文章提出的新模型训练的生成器G刚好是编码器E的反过来，整个模型是由自上而下的GAN组合而成，每个GAN都生成了表示高级表示的低级特征。同时引入了一个条件损失，它鼓励使用来自上面一层的条件信息，以及一个新的熵损失，他可以最大化G输出的条件熵损失。"></a>&emsp;&emsp;这篇文章提出的新模型训练的生成器G刚好是编码器E的反过来，整个模型是由自上而下的GAN组合而成，每个GAN都生成了表示高级表示的低级特征。同时引入了一个条件损失，它鼓励使用来自上面一层的条件信息，以及一个新的熵损失，他可以最大化G输出的条件熵损失。</h4><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1、论文全名-—-Stacked-Generative-Adversarial-Networks&quot;&gt;&lt;a href=&quot;#1、论文全名-—-Stacked-Generative-Adversarial-Networks&quot; class=&quot;headerlink&quot; tit</summary>
      
    
    
    
    <category term="论文阅读（第一层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    <category term="GAN（第二层级）" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/GAN%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    
    <category term="GAN，论文阅读" scheme="http://example.com/tags/GAN%EF%BC%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>我的第一个博客(Typora基础的使用技巧)</title>
    <link href="http://example.com/2021/07/31/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://example.com/2021/07/31/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-07-31T07:00:00.000Z</published>
    <updated>2021-08-03T08:03:00.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id><a href="#" class="headerlink" title></a></h2><h2 id="大概花费了2天时间才把hexo框架搭好、刚刚下载了一个Typora编辑器来练练手熟悉一下操作。"><a href="#大概花费了2天时间才把hexo框架搭好、刚刚下载了一个Typora编辑器来练练手熟悉一下操作。" class="headerlink" title="大概花费了2天时间才把hexo框架搭好、刚刚下载了一个Typora编辑器来练练手熟悉一下操作。"></a>大概花费了2天时间才把hexo框架搭好、刚刚下载了一个Typora编辑器来练练手熟悉一下操作。</h2><hr><h2 id="以下是具体的一些基础使用方法。"><a href="#以下是具体的一些基础使用方法。" class="headerlink" title="以下是具体的一些基础使用方法。"></a>以下是具体的一些基础使用方法。</h2><h2 id="1-常用的语法和快捷键"><a href="#1-常用的语法和快捷键" class="headerlink" title="1.常用的语法和快捷键"></a>1.常用的语法和快捷键</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"> Ctrl+1  一阶标题                        Ctrl+B  字体加粗</span><br><span class="line"></span><br><span class="line"> Ctrl+2  二阶标题                        Ctrl+I  字体倾斜</span><br><span class="line"></span><br><span class="line"> Ctrl+3  三阶标题                        Ctrl+U  下划线</span><br><span class="line"></span><br><span class="line"> Ctrl+4  四阶标题                        Ctrl+Home   返回Typora顶部 </span><br><span class="line"></span><br><span class="line">Ctrl+5  五阶标题    Ctrl+End    返回Typora底部 </span><br><span class="line"></span><br><span class="line">Ctrl+6  六阶标题    Ctrl+T  创建表格 </span><br><span class="line"></span><br><span class="line">Ctrl+L  选中某句话     Ctrl+D  选中某个单词 </span><br><span class="line"></span><br><span class="line"> Ctrl+F  搜索                Ctrl+E  选中相同格式的文字   </span><br><span class="line"></span><br><span class="line"> Ctrl+H  搜索并替换                       Alt+Shift+5 删除线</span><br><span class="line"></span><br><span class="line"> Ctrl+Shift+I    插入图片                 Ctrl+Shift+M   公式块 </span><br><span class="line"> </span><br><span class="line"> Ctrl+K（先复制链接，然后选中要加链接的文本，按快捷键。Ctrl+左键点击文本可跳转到对应链接） </span><br><span class="line"> </span><br><span class="line">`代码片段：Ctrl+Shift+ ` </span><br><span class="line"></span><br><span class="line"> 代码块：任意位置按 Ctrl+Shift+K ，然后选择语言然后在代码块中输入代码</span><br></pre></td></tr></table></figure><h4 id="字体颜色的切换："><a href="#字体颜色的切换：" class="headerlink" title="字体颜色的切换："></a>字体颜色的切换：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;span style=<span class="string">&quot;color:red&quot;</span>&gt;这是一个红色&lt;/span&gt;</span><br><span class="line">&lt;span style=<span class="string">&quot;color:blue&quot;</span>&gt;这是一个蓝色&lt;/span&gt;</span><br><span class="line">&lt;span style=<span class="string">&quot;color:green&quot;</span>&gt;这是一个绿色&lt;/span&gt;</span><br><span class="line">&lt;span style=<span class="string">&quot;color:yellow&quot;</span>&gt;这是一个黄色&lt;/span&gt;</span><br></pre></td></tr></table></figure><p><span style="color:red">这是一个红色</span><br><span style="color:blue">这是一个蓝色</span><br><span style="color:green">这是一个绿色</span><br><span style="color:yellow">这是一个黄色</span></p><hr><h2 id="2-列表的生成"><a href="#2-列表的生成" class="headerlink" title="2.列表的生成"></a>2.列表的生成</h2><h4 id="普通列表："><a href="#普通列表：" class="headerlink" title="普通列表："></a>普通列表：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* 样式<span class="number">1</span></span><br><span class="line">+ 样式<span class="number">2</span></span><br><span class="line">* 加符号前按` Tab `键有缩进效果</span><br><span class="line">    + 第一项</span><br><span class="line">    - 第二项</span><br></pre></td></tr></table></figure><ul><li>这是第一项<ul><li>​    加tab即可<ul><li>继续加tab</li></ul></li></ul></li><li>第二项</li></ul><h4 id="任务列表："><a href="#任务列表：" class="headerlink" title="任务列表："></a>任务列表：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- [ ] <span class="number">7</span>:<span class="number">00</span> 起床</span><br><span class="line">- [x] <span class="number">8</span>:<span class="number">00</span> 吃饭</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><ul><li><input disabled type="checkbox"> 7:00 起床</li><li><input checked disabled type="checkbox"> 8:00 吃饭</li></ul><hr><h2 id="3-语法生成"><a href="#3-语法生成" class="headerlink" title="3.语法生成"></a>3.语法生成</h2><h4 id="3-1-表格语句"><a href="#3-1-表格语句" class="headerlink" title="3.1 表格语句"></a>3.1 表格语句</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|姓名|性别|年龄|手机号|</span><br><span class="line">|:---|:--:|:--:|---:|</span><br><span class="line">|张三|男|<span class="number">21</span>|<span class="number">12345678909</span>|</span><br><span class="line">|李四|女|<span class="number">23</span>|<span class="number">12345678908</span>|</span><br><span class="line">|王五|男|<span class="number">25</span>|<span class="number">12345678907</span>|</span><br></pre></td></tr></table></figure><p>生成效果如下:</p><table><thead><tr><th align="left">姓名</th><th align="center">性别</th><th align="center">年龄</th><th align="right">手机号</th></tr></thead><tbody><tr><td align="left">张三</td><td align="center">男</td><td align="center">21</td><td align="right">12345678909</td></tr><tr><td align="left">李四</td><td align="center">女</td><td align="center">23</td><td align="right">12345678908</td></tr><tr><td align="left">王五</td><td align="center">男</td><td align="center">25</td><td align="right">12345678907</td></tr></tbody></table><h4 id="3-2-区块"><a href="#3-2-区块" class="headerlink" title="3.2 区块"></a>3.2 区块</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; 这是一个区块。</span><br><span class="line">&gt; 网上也有人称为引用。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">区块外层</span><br><span class="line">  &gt; 嵌套<span class="number">0</span></span><br><span class="line">  &gt;&gt; 嵌套<span class="number">1</span></span><br><span class="line">  &gt;&gt;&gt; 嵌套<span class="number">2</span></span><br></pre></td></tr></table></figure><p>生成效果如下:</p><blockquote><p>这是一个区块。<br>网上也有人称为引用。</p></blockquote><p>区块外层</p><blockquote><p>嵌套0</p><blockquote><p>嵌套1</p><blockquote><p>嵌套2</p></blockquote></blockquote></blockquote><hr><h2 id="4-引用"><a href="#4-引用" class="headerlink" title="4.引用"></a>4.引用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27; 引用网址：[显示名称](跳转链接) &#x27;</span></span><br><span class="line">[简书](https://www.jianshu.com/)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27; 引用图片网址链接：！[图象名称](网址/也可以是当前计算机图片路径) &#x27;</span></span><br><span class="line"><span class="string">&#x27; ！[图片名称要不要无所谓，填写会显示](网址/路径是必须要的！)  &#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27; 引用视频链接：&lt;video src=&quot;xxx.mp4&quot; /&gt; (视频不会自动播放!) &#x27;</span></span><br><span class="line">&lt;video src=<span class="string">&quot;https://typora.io/img/beta.mp4&quot;</span>/&gt;</span><br></pre></td></tr></table></figure><p>‘ 引用网址：<a href="%E8%B7%B3%E8%BD%AC%E9%93%BE%E6%8E%A5">显示名称</a> ‘<br><a href="https://www.jianshu.com/">简书</a></p><p>‘ 引用图片网址链接：！<a href="https://www.macz.com/desk/1245.html">孙悟空</a> ‘<br>‘ ！<a href="%E7%BD%91%E5%9D%80/%E8%B7%AF%E5%BE%84%E6%98%AF%E5%BF%85%E9%A1%BB%E8%A6%81%E7%9A%84%EF%BC%81">图片名称要不要无所谓，填写会显示</a>  ‘</p><p>‘ 引用视频链接：<video src="xxx.mp4"> (视频不会自动播放!) ‘<br><video src="https://typora.io/img/beta.mp4"></video></video></p><hr><h2 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a><a href="https://www.jianshu.com/p/b9bba08b4dd3">参考网站</a></h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2 id=&quot;大概花费了2天时间才把hexo框架搭好、刚刚下载了一个Typora编辑器来练练手熟悉一下操作。&quot;&gt;&lt;a href=&quot;#大概花费了2天时间才把hexo框架搭好、</summary>
      
    
    
    
    <category term="个人博客（第一层级）" scheme="http://example.com/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    <category term="Hexo博客（第二层级）" scheme="http://example.com/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%A7%EF%BC%89/Hexo%E5%8D%9A%E5%AE%A2%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%A7%EF%BC%89/"/>
    
    
    <category term="Typora基础的使用技巧" scheme="http://example.com/tags/Typora%E5%9F%BA%E7%A1%80%E7%9A%84%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
</feed>
